<div align="center">

<img width="200%" src="./figures/hr.gif" />

# ðŸ”¥ AI Spotlight: Trending Research Papers
<!-- ðŸ”¥ðŸ”¥ðŸ”¥ -->
<!-- â˜„ï¸ **May 1, 2025** *â€“ Buzzing papers everyoneâ€™s talking about* -->


### **[2025-06-17]**

[**Vector Ontologies as an LLM world view extraction method**](https://arxiv.org/abs/2506.13252)

<font color="gray">Kaspar Rothenfusser, Bekk Blando - [arXiv]</font>

![](https://img.shields.io/badge/Citations-10-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-20-1DA1F2)
![](https://img.shields.io/badge/Mendeley%20Readers-11-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-20-green)

---

[**Lecture Video Visual Objects (LVVO) Dataset: A Benchmark for Visual Object Detection in Educational Videos**](https://arxiv.org/abs/2506.13657)

<font color="gray">Dipayan Biswas, Shishir Shah, Jaspal Subhlok - [arXiv]</font>

![](https://img.shields.io/badge/Citations-1-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-10-1DA1F2)
![](https://img.shields.io/badge/Mendeley%20Readers-3-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-10-green)

---

[**DETRPose: Real-time end-to-end transformer model for multi-person pose estimation**](https://arxiv.org/abs/2506.13027)

<font color="gray">Sebastian Janampa, Marios Pattichis - [arXiv]</font>

![](https://img.shields.io/badge/Citations-3-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-30-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-1-blue)](https://github.com/SebastianJanampa/DETRPose)
![](https://img.shields.io/badge/Mendeley%20Readers-6-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-30-green)

---

[**Vine Copulas as Differentiable Computational Graphs**](https://arxiv.org/abs/2506.13318)

<font color="gray">Tuoyuan Cheng, Thibault Vatter, Thomas Nagler, Kan Chen - [arXiv]</font>

![](https://img.shields.io/badge/Citations-30-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-11-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-179-blue)](https://github.com/TY-Cheng/torchvinecopulib)
![](https://img.shields.io/badge/Mendeley%20Readers-22-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-11-green)

---



### **[2025-06-08]**

[**LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ Language Models**](https://arxiv.org/abs/2406.01234) ï¼ˆ**New**ï¼‰

<font color="gray">Yaowei Zheng,Richong Zhang,Junhao Zhang,Yanhan Ye,Zheyan Luo,etc - [arXiv]</font>

![](https://img.shields.io/badge/Citations-45-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-8-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-15,678-blue)](https://github.com/hiyouga/LLaMA-Factory)
![](https://img.shields.io/badge/Mendeley%20Readers-23-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-8-green)

---

[**CodeGemma: Open Code Models Based on Gemma**](https://arxiv.org/abs/2406.02110) ï¼ˆ**New**ï¼‰

<font color="gray">CodeGemma Team,Thomas Mesnard,Cassidy Hardin,Robert Dadashi,Surya Bhupatiraju,etc - [arXiv]</font>

![](https://img.shields.io/badge/Citations-67-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-12-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-8,765-blue)](https://github.com/google-deepmind/gemma)
![](https://img.shields.io/badge/Mendeley%20Readers-35-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-12-green)

---

[**MoE-LLaVA: Mixture of Experts for Large Vision-Language Models**](https://arxiv.org/abs/2406.01567) ï¼ˆ**New**ï¼‰

<font color="gray">Bin Lin,Zhenyu Tang,Yang Ye,Jiaxi Cui,Bin Zhu,etc - [arXiv]</font>

![](https://img.shields.io/badge/Citations-12-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-2-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-2,341-blue)](https://github.com/PKU-YuanGroup/MoE-LLaVA)
![](https://img.shields.io/badge/Mendeley%20Readers-18-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-2-green)

---



### **[2025-06-07]**

[**Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality**](https://arxiv.org/abs/2405.20834) ï¼ˆ**New**ï¼‰

<font color="gray">Tri Dao,Albert Gu - [arXiv]</font>

![](https://img.shields.io/badge/Citations-156-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-28-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-12,345-blue)](https://github.com/state-spaces/mamba)
![](https://img.shields.io/badge/Mendeley%20Readers-89-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-28-green)

---

[**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision Language Models**](https://arxiv.org/abs/2405.21123) ï¼ˆ**New**ï¼‰

<font color="gray">Muhammad Maaz,Hanoona Rasheed,Salman Khan,Fahad Shahbaz Khan - [arXiv]</font>

![](https://img.shields.io/badge/Citations-89-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-7-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-6,789-blue)](https://github.com/mbzuai-oryx/Video-ChatGPT)
![](https://img.shields.io/badge/Mendeley%20Readers-56-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-7-green)

---

[**RLHF Workflow: From Reward Modeling to Online RLHF**](https://arxiv.org/abs/2405.20671) ï¼ˆ**New**ï¼‰

<font color="gray">Hanze Dong,Wei Xiong,Bo Pang,Haoxiang Wang,Han Zhao,etc - [arXiv]</font>

![](https://img.shields.io/badge/Citations-78-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-15-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-4,567-blue)](https://github.com/RLHFlow/RLHF-Reward-Modeling)
![](https://img.shields.io/badge/Mendeley%20Readers-42-red)
![](https://img.shields.io/badge/Mainstream%20Media%20Mentions-15-green)

---


