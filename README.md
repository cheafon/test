<div align="center">

<img width="200%" src="./figures/hr.gif" />

# ðŸ”¥ AI Spotlight: Trending Research Papers
<!-- ðŸ”¥ðŸ”¥ðŸ”¥ -->
<!-- â˜„ï¸ **May 1, 2025** *â€“ Buzzing papers everyoneâ€™s talking about* -->


### **[2025-06-08]**

[**LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ Language Models**](https://arxiv.org/abs/2406.01234) ï¼ˆ**New**ï¼‰

<font color="gray">Yaowei Zheng,Richong Zhang,Junhao Zhang,Yanhan Ye,Zheyan Luo,etc - [arXiv]</font>

![](https://img.shields.io/badge/Citations-45-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-256-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-15,678-blue)](https://github.com/hiyouga/LLaMA-Factory)
![](https://img.shields.io/badge/Mendeley%20Readers-23-red)
![](https://img.shields.io/badge/News%20Mentions-8-green)

---

[**CodeGemma: Open Code Models Based on Gemma**](https://arxiv.org/abs/2406.02110) ï¼ˆ**New**ï¼‰

<font color="gray">CodeGemma Team,Thomas Mesnard,Cassidy Hardin,Robert Dadashi,Surya Bhupatiraju,etc - [arXiv]</font>

![](https://img.shields.io/badge/Citations-67-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-423-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-8,765-blue)](https://github.com/google-deepmind/gemma)
![](https://img.shields.io/badge/Mendeley%20Readers-35-red)
![](https://img.shields.io/badge/News%20Mentions-12-green)

---

[**MoE-LLaVA: Mixture of Experts for Large Vision-Language Models**](https://arxiv.org/abs/2406.01567) ï¼ˆ**New**ï¼‰

<font color="gray">Bin Lin,Zhenyu Tang,Yang Ye,Jiaxi Cui,Bin Zhu,etc - [arXiv]</font>

![](https://img.shields.io/badge/Citations-12-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-98-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-2,341-blue)](https://github.com/PKU-YuanGroup/MoE-LLaVA)
![](https://img.shields.io/badge/Mendeley%20Readers-18-red)
![](https://img.shields.io/badge/News%20Mentions-2-green)

---



### **[2025-06-07]**

[**Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality**](https://arxiv.org/abs/2405.20834) ï¼ˆ**New**ï¼‰

<font color="gray">Tri Dao,Albert Gu - [arXiv]</font>

![](https://img.shields.io/badge/Citations-156-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-567-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-12,345-blue)](https://github.com/state-spaces/mamba)
![](https://img.shields.io/badge/Mendeley%20Readers-89-red)
![](https://img.shields.io/badge/News%20Mentions-28-green)

---

[**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision Language Models**](https://arxiv.org/abs/2405.21123) ï¼ˆ**New**ï¼‰

<font color="gray">Muhammad Maaz,Hanoona Rasheed,Salman Khan,Fahad Shahbaz Khan - [arXiv]</font>

![](https://img.shields.io/badge/Citations-89-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-234-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-6,789-blue)](https://github.com/mbzuai-oryx/Video-ChatGPT)
![](https://img.shields.io/badge/Mendeley%20Readers-56-red)
![](https://img.shields.io/badge/News%20Mentions-7-green)

---

[**RLHF Workflow: From Reward Modeling to Online RLHF**](https://arxiv.org/abs/2405.20671) ï¼ˆ**New**ï¼‰

<font color="gray">Hanze Dong,Wei Xiong,Bo Pang,Haoxiang Wang,Han Zhao,etc - [arXiv]</font>

![](https://img.shields.io/badge/Citations-78-9cf)
![](https://img.shields.io/badge/Twitter%20Mentions-345-1DA1F2)
[![](https://img.shields.io/badge/GitHub%20Stars-4,567-blue)](https://github.com/RLHFlow/RLHF-Reward-Modeling)
![](https://img.shields.io/badge/Mendeley%20Readers-42-red)
![](https://img.shields.io/badge/News%20Mentions-15-green)

---


